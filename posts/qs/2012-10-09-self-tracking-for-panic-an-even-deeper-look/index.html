<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Self-Tracking For Panic: A Deeper Look</title>
    <meta name="description" content="Evan Savage&#39;s personal blog about software, life, travel, and other sundry things.">
    <link href='https://fonts.googleapis.com/css?family=PT%20Serif:400,400italic,500,500italic' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="/css/index.css">
    <link rel="stylesheet" href="/css/prism-base16-monokai.dark.css">
    <link rel="alternate" href="/feed/feed.xml" type="application/atom+xml" title="Evan Savage // Blog Archive">
    <link rel="alternate" href="/feed/feed.json" type="application/json" title="Evan Savage // Blog Archive">

    <!-- MathJax -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
        },
      };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-07YLDJQ67V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-07YLDJQ67V');
    </script>
  </head>
  <body>
    <header>
      <div class="home"><a href="/">Evan Savage // Blog Archive</a></div>

      <div class="spacer"></div>

      <ul class="nav">
        <li class="nav-item">
          <a href="https://savageevan.com">New Blog!</a>
        </li>
      </ul>
    </header>

    <main class="tmpl-post">
      <h1>Self-Tracking For Panic: A Deeper Look</h1>

<p>In this post, I apply three statistical and machine learning tools to my panic recovery journal data: linear regression/correlation, the Fast Fourier Transform, and maximum entropy modelling.</p>
<!-- more -->
<h2 id="first%2C-a-word-about-tools">First, A Word About Tools <a class="direct-link" href="#first%2C-a-word-about-tools">#</a></h2>
<blockquote cite="https://www.worldcat.org/title/psychology-of-science-a-reconnaissance/oclc/248935">
  <p>
I suppose it is tempting, if the only tool you have is a hammer, to treat
everything as if it were a nail.
</p>
  <footer>&ndash; Abraham Maslow, <cite><a href="https://www.worldcat.org/title/psychology-of-science-a-reconnaissance/oclc/248935">The Psychology of Science: A Reconnaissance</a></footer>
</blockquote>
<h2 id="now%2C-a-necessary-disclaimer">Now, A Necessary Disclaimer <a class="direct-link" href="#now%2C-a-necessary-disclaimer">#</a></h2>
<p>My experiment has fewer than 50 samples, which is <em>nowhere near enough to draw statistically significant conclusions</em>. That's not the point. The primary purpose of this post is to <em>demonstrate analysis techniques by example</em>. These same methods can be wielded on larger datasets, where they are much more useful.</p>
<h2 id="getting-ready">Getting Ready <a class="direct-link" href="#getting-ready">#</a></h2>
<p>To follow along with the examples here, you'll need the excellent Python toolkits <a href="http://scipy.org/">scipy</a>, <a href="http://matplotlib.org/">matplotlib</a>, and <a href="http://nltk.org/">nltk</a>:</p>
<pre class="language-bash"><code class="language-bash">$ pip <span class="token function">install</span> scipy nltk matplotlib</code></pre>
<h2 id="linear-regression">Linear Regression <a class="direct-link" href="#linear-regression">#</a></h2>
<h3 id="what%3F">What? <a class="direct-link" href="#what%3F">#</a></h3>
<p>Linear regression answers this question:</p>
<blockquote>
What is the line that most closely fits this data?
</blockquote>
<p>Given points $ P_i = (x_i, y_i) $, the goal is to find the line $ y = mx + b $ such that some error function is minimized. A common one is the least squares function:</p>
<p>$$<br>
f(m, b) = \sum_{i} \left(y_i - (mx_i + b)\right)^2<br>
$$</p>
<p>The <a href="http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient">Pearson correlation coefficient</a> $ R $ and <a href="http://www.lstr.net/blog/2008/07/08/p-values-explained-well/">p-value</a> $ p $ are also useful here, as they measure <em>correlation</em> and <em>statistical significance</em>.</p>
<h3 id="why%3F">Why? <a class="direct-link" href="#why%3F">#</a></h3>
<p>In a self-tracking context, you might ask the following questions:</p>
<ul>
<li>Have I been exercising more over time?</li>
<li>Does exercise affect mood? By how much and in what direction?</li>
</ul>
<p>Linear regression can help address both questions. However, it can only find <em>linear</em> relationships between datasets. Many dynamic processes are <em>locally linear</em> but not <em>globally linear</em>. For instance, there are practical limits to how much you can exercise in a day, so no linear model with non-zero slope will accurately capture your exercise duration for all time.</p>
<h3 id="the-data">The Data <a class="direct-link" href="#the-data">#</a></h3>
<p>You can see the code for this analysis <a href="https://github.com/candu/quantified-savagery-files/blob/master/Panic/recovery-journal/food_linregress.py">here</a>. I look at only the first 31 days, that being the largest consecutive run for which I have data.</p>
<img src="https://lh6.googleusercontent.com/-plD2webhfrY/UHXc4xHxAGI/AAAAAAAAACM/2X488DqHKms/s640/alcohol.jpg" alt="Alcohol Consumption" />
<p>My alcohol consumption did not decrease over time, but rather stayed fairly constant: with $ R = 0.0098 $, there is no correlation between alcohol and time.</p>
<img src="https://lh5.googleusercontent.com/-UCZKlx5l5RI/UHXc6u8h2vI/AAAAAAAAACs/CWcJjS09dS8/s640/sweets.jpg" alt="Sugar Consumption" />
<p>Sugar consumption is a similar story: although the best-fit slope is slightly negative, $ R = -0.0671 $ indicates no correlation over time. It seems that my alcohol and sugar consumption were not modified significantly over the tracking period.</p>
<img src="https://lh5.googleusercontent.com/-Ssz89uoE-EA/UHXc5DvHf0I/AAAAAAAAACU/o0C_PJpmZcM/s640/alcohol-and-sugar.jpg" alt="Alcohol and Sugar Consumption" />
<p>I decided to graph alcohol and sugar together. It looks like they might be related, as the peaks in each seem to coincide on several occasions. Let's test this hypothesis:</p>
<img src="https://lh6.googleusercontent.com/-iCO9umA8L8s/UHXc5vImvhI/AAAAAAAAACc/d82SCqFs-qI/s640/alcohol-today-vs-yesterday.jpg" alt="Alcohol vs. Sugar Consumption" />
<p>The positive slope is more pronounced this time, but $ R = 0.1624 $ still indicates a small degree of correlation. We can also look at the p-value: with $ p = 0.3827 $, it is fairly easy to write this off as a random effect.</p>
<p>Finally, let's take another look at a question from <a href="/posts/2012-10-08-self-tracking-for-panic-a-deeper-look/">a previous blog post</a>:</p>
<blockquote>
On days where I drink heavily, do I drink less the day after?
</blockquote>
<img src="https://lh6.googleusercontent.com/-iCO9umA8L8s/UHXc5vImvhI/AAAAAAAAACc/d82SCqFs-qI/s640/alcohol-today-vs-yesterday.jpg" alt="Alcohol Consumption: Today vs. Yesterday" />
<p>There's a negative slope there, but the correlation and p-value statistics are in the same uncertain zone as before. I likely need more data to investigate these last two effects properly.</p>
<h2 id="fast-fourier-transform">Fast Fourier Transform <a class="direct-link" href="#fast-fourier-transform">#</a></h2>
<h3 id="what%3F-2">What? <a class="direct-link" href="#what%3F-2">#</a></h3>
<p>Fourier analysis answers this question:</p>
<blockquote>
What frequencies comprise this signal?
</blockquote>
<p>Given a sequence $ x_n $, a <a href="http://en.wikipedia.org/wiki/Discrete_Fourier_transform">Discrete Fourier Transform</a> (DFT) computes</p>
<p>$$<br>
X_k = \sum_{n=0}^{N-1} x_n \cdot e^{\frac{-2 i \pi k n}{N}}<br>
$$</p>
<p>The $ X_k $ encode the amplitude and phase of frequencies $ \frac{f k}{N} $ Hz, where $ T $ is the time between samples and $ f = 1 / T $ is the sampling frequency.</p>
<p>As described here, the DFT requires $ \mathcal{O}(N^2) $ time to compute. The <a href="http://mathworld.wolfram.com/FastFourierTransform.html">Fast Fourier Transform</a> (FFT) uses divide-and-conquer on this sum of complex exponentials to compute the DFT in $ \mathcal{O}(N \log N) $ time. <a href="http://groups.csail.mit.edu/netmit/sFFT/">Further speedups are possible</a> for real-world signals that are sparse in the frequency domain.</p>
<h3 id="why%3F-2">Why? <a class="direct-link" href="#why%3F-2">#</a></h3>
<p>In a self-tracking context, you might ask the following questions:</p>
<ul>
<li>Do I have regular exercising patterns?</li>
<li>Do these patterns cycle weekly? bi-weekly? monthly?</li>
<li>How much does my amount of exercise fluctuate during a cycle?</li>
</ul>
<p>With the FFT, Fourier analysis can help address these questions. However, it can only find <em>periodic</em> effects. Unlike linear regression, it does not help find <em>trends</em> in your data.</p>
<h3 id="the-data-2">The Data <a class="direct-link" href="#the-data-2">#</a></h3>
<p>You can see the code for this analysis <a href="https://github.com/candu/quantified-savagery-files/blob/master/Panic/recovery-journal/food_fft.py">here</a>. Again, I look at the first 31 days to ensure that the frequency analysis is meaningful.</p>
<img src="https://lh5.googleusercontent.com/-8j00ob_Ji-Y/UHXc67MQVpI/AAAAAAAAAC0/n3akVSjtRHs/s640/fft-frequencies.jpg" alt="Frequency Strengths" />
<p>There are some apparent maxima there, but it's hard to tell what they mean. Part of the difficulty is that <em>these are frequencies rather than period lengths</em>, so let's deal with that:</p>
<pre class="language-bash"><code class="language-bash">$ python food_fft.py<br>food_fft.py:32: RuntimeWarning: divide by zero encountered <span class="token keyword">in</span> divide<br>  <span class="token keyword">for</span> strength, phase, period <span class="token keyword">in</span> sorted<span class="token punctuation">(</span>zip<span class="token punctuation">(</span>FS, FP, <span class="token number">1.0</span> / Q<span class="token punctuation">))</span><span class="token punctuation">[</span>-5:<span class="token punctuation">]</span>:<br><span class="token punctuation">[</span><span class="token number">2.21</span> days<span class="token punctuation">]</span> <span class="token number">3.0461</span> <span class="token punctuation">(</span>phase<span class="token operator">=</span>-0.67 days<span class="token punctuation">)</span><br><span class="token punctuation">[</span>-2.21 days<span class="token punctuation">]</span> <span class="token number">3.0461</span> <span class="token punctuation">(</span>phase<span class="token operator">=</span>-0.67 days<span class="token punctuation">)</span><br><span class="token punctuation">[</span><span class="token number">7.75</span> days<span class="token punctuation">]</span> <span class="token number">3.1116</span> <span class="token punctuation">(</span>phase<span class="token operator">=</span>-3.67 days<span class="token punctuation">)</span><br><span class="token punctuation">[</span>-7.75 days<span class="token punctuation">]</span> <span class="token number">3.1116</span> <span class="token punctuation">(</span>phase<span class="token operator">=</span>-3.67 days<span class="token punctuation">)</span><br>food_fft.py:33: RuntimeWarning: invalid value encountered <span class="token keyword">in</span> double_scalars<br>  phase_days <span class="token operator">=</span> period * <span class="token punctuation">(</span>phase / <span class="token punctuation">(</span><span class="token number">2.0</span> * math.pi<span class="token punctuation">))</span><br><span class="token punctuation">[</span>inf days<span class="token punctuation">]</span> <span class="token number">18.1401</span> <span class="token punctuation">(</span>phase<span class="token operator">=</span>nan days<span class="token punctuation">)</span></code></pre>
<p>If you're not familiar with the Fourier transform, the last line might be a bit mysterious. That corresponds to $ X_0 $, which is just the sum of the original samples:</p>
<p>$$<br>
X_0 = \sum_{n=0}^{N-1} x_n \cdot e^0 = \sum_{n=0}^{N-1} x_n<br>
$$</p>
<p>Other than that, the most pronounced cycles have period lengths of 2.21 days and 7.75 days. The former might be explained by a <em>see-saw drinking pattern</em>, whereas the latter is likely related to the day-of-week effects we saw <a href="/posts/2012-10-08-self-tracking-for-panic-a-deeper-look/">in the previous post</a>.</p>
<p>Which day of the week? The phase is -3.67 days, and our sample starts on a Monday, placing the first peak on Thursday. The period is slightly longer than a week, though, and the data runs for 31 days, so these peaks gradually shift to <em>cover the weekend</em>.</p>
<p>There are two caveats:</p>
<ol>
<li>I have no idea whether a Fourier coefficient of about 3 is significant here. If it isn't, I'm grasping at straws.</li>
<li>Again, the small amount of data means the frequency domain data is sparse. To accurately test for bi-daily or weekly effects, I <em>need more fine-grained period lengths.</em></li>
</ol>
<h2 id="maximum-entropy-modelling">Maximum Entropy Modelling <a class="direct-link" href="#maximum-entropy-modelling">#</a></h2>
<h3 id="what%3F-3">What? <a class="direct-link" href="#what%3F-3">#</a></h3>
<p>Maximum entropy modelling answers this question:</p>
<blockquote>
Given observations of a random process, what is the most likely model
for that random process?
</blockquote>
<p>Given a discrete probability distribution $ p(X = x_k) = p_k $, the entropy of this distribution is given by</p>
<p>$$<br>
H(p) = \sum - p_k \log p_k<br>
$$</p>
<p>(Yes, I'm conflating the concepts of <a href="http://en.wikipedia.org/wiki/Random_variable">random variables</a> and <a href="http://en.wikipedia.org/wiki/Probability_distribution">probability distributions</a>. If you knew that, you probably don't need this explanation.)</p>
<p>This can be thought of as the <em>number of bits needed to encode outcomes in this distribution</em>. For instance, if I have a double-headed coin, I need no bits: I already know the outcome. Given a fair coin, though, I need one bit: heads or tails?</p>
<p>After repeated sampling, we get observed expected values for $ p_k $; let these be $ p'_k $. Since we would like the model to <em>accurately reflect what we already know</em>, we impose the constraints $ p_k = p'_k $. The maximum entropy model is the model that also maximizes $ H(p') $.</p>
<p>This model encodes what is known <em>while remaining maximally noncommittal on what is unknown.</em></p>
<p>Adam Berger (CMU) provides <a href="http://www.cs.cmu.edu/afs/cs/user/aberger/www/html/tutorial/node2.html#SECTION00011000000000000000">a more concrete example</a>. If you're interested in learning more, his tutorial is highly recommended reading.</p>
<h3 id="why%3F-3">Why? <a class="direct-link" href="#why%3F-3">#</a></h3>
<p>In a self-tracking context, you might ask the following questions:</p>
<ul>
<li>Which treatments have the greatest effect in preventing panic attacks? Which have the least effect?</li>
<li>Today I exercised for at least 30 minutes and had four drinks. Am I likely to get a panic attack?</li>
<li>What treatments should I try next?</li>
</ul>
<p>Maximum entropy modelling can help address these questions. It is often used to <em>classify unseen examples</em>, and would be fantastic in a <a href="http://100plus.com/2012/09/qs-data-commons/">data commons</a> scenario with enough data to provide recommendations to users.</p>
<h3 id="feature-extraction">Feature Extraction <a class="direct-link" href="#feature-extraction">#</a></h3>
<p>Since I'm now effectively building a classifier, there's an additional step. I need features for my classifier, which I extract from my existing datasets:</p>
<pre class="language-py"><code class="language-py">train_set <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><br>dates <span class="token operator">=</span> <span class="token builtin">set</span><span class="token punctuation">(</span>W<span class="token punctuation">)</span><span class="token punctuation">.</span>intersection<span class="token punctuation">(</span>F<span class="token punctuation">)</span><br><span class="token keyword">for</span> ds <span class="token keyword">in</span> dates<span class="token punctuation">:</span><br>  <span class="token keyword">try</span><span class="token punctuation">:</span><br>    ds_data <span class="token operator">=</span> <span class="token punctuation">{</span><br>      <span class="token string">'relaxation'</span> <span class="token punctuation">:</span> <span class="token builtin">bool</span><span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">(</span>W<span class="token punctuation">[</span>ds<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'relaxation'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>      <span class="token string">'exercise'</span> <span class="token punctuation">:</span> <span class="token builtin">bool</span><span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">(</span>W<span class="token punctuation">[</span>ds<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'exercise'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span><br>      <span class="token string">'caffeine'</span> <span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">(</span>F<span class="token punctuation">[</span>ds<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'caffeine'</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">,</span><br>      <span class="token string">'sweets'</span> <span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">(</span>F<span class="token punctuation">[</span>ds<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'sweets'</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">></span> <span class="token number">1</span><span class="token punctuation">,</span><br>      <span class="token string">'alcohol'</span> <span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">(</span>F<span class="token punctuation">[</span>ds<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'alcohol'</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">></span> <span class="token number">4</span><span class="token punctuation">,</span><br>      <span class="token string">'supplements'</span> <span class="token punctuation">:</span> <span class="token builtin">bool</span><span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">(</span>F<span class="token punctuation">[</span>ds<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'supplements'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><br>    <span class="token punctuation">}</span><br>  <span class="token keyword">except</span> <span class="token punctuation">(</span>ValueError<span class="token punctuation">,</span> KeyError<span class="token punctuation">)</span><span class="token punctuation">:</span><br>    <span class="token keyword">continue</span><br>  had_panic <span class="token operator">=</span> P<span class="token punctuation">.</span>get<span class="token punctuation">(</span>ds<span class="token punctuation">)</span> <span class="token keyword">and</span> <span class="token string">'panic'</span> <span class="token keyword">or</span> <span class="token string">'no-panic'</span><br>  train_set<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">(</span>ds_data<span class="token punctuation">,</span> had_panic<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre>
<p>Note that the features listed here are binary. I use my daily goals as thresholds on caffeine, sweets, and alcohol.</p>
<p>(If you know how to get float-valued features working with NLTK, let me know! Otherwise, there's always <a href="http://www.cs.utah.edu/~hal/megam/">megam</a> or <a href="http://www-i6.informatik.rwth-aachen.de/web/Software/YASMET.html">YASMET</a>.</p>
<h3 id="the-data-3">The Data <a class="direct-link" href="#the-data-3">#</a></h3>
<p>You can see the code for this analysis <a href="https://github.com/candu/quantified-savagery-files/blob/master/Panic/recovery-journal/panic_maxent.py">here</a>. This time I don't care about having consecutive dates, so I use all of the samples.</p>
<p>After building a <code>MaxentClassifier</code>, I print out the most informative features with <code>show_most_informative_features()</code>:</p>
<pre class="language-bash"><code class="language-bash">  <span class="token parameter variable">-2.204</span> <span class="token assign-left variable">exercise</span><span class="token operator">==</span>True and label is <span class="token string">'panic'</span><br>   <span class="token number">1.821</span> <span class="token assign-left variable">caffeine</span><span class="token operator">==</span>True and label is <span class="token string">'panic'</span><br>  <span class="token parameter variable">-0.867</span> <span class="token assign-left variable">relaxation</span><span class="token operator">==</span>True and label is <span class="token string">'panic'</span><br>   <span class="token number">0.741</span> <span class="token assign-left variable">alcohol</span><span class="token operator">==</span>True and label is <span class="token string">'panic'</span><br>  <span class="token parameter variable">-0.615</span> <span class="token assign-left variable">caffeine</span><span class="token operator">==</span>True and label is <span class="token string">'no-panic'</span><br>  <span class="token parameter variable">-0.537</span> <span class="token assign-left variable">supplements</span><span class="token operator">==</span>True and label is <span class="token string">'panic'</span><br>   <span class="token number">0.439</span> <span class="token assign-left variable">sweets</span><span class="token operator">==</span>True and label is <span class="token string">'panic'</span><br>   <span class="token number">0.430</span> <span class="token assign-left variable">exercise</span><span class="token operator">==</span>True and label is <span class="token string">'no-panic'</span><br>   <span class="token number">0.284</span> <span class="token assign-left variable">relaxation</span><span class="token operator">==</span>True and label is <span class="token string">'no-panic'</span><br>   <span class="token number">0.233</span> <span class="token assign-left variable">supplements</span><span class="token operator">==</span>True and label is <span class="token string">'no-panic'</span></code></pre>
<p>Exercise, relaxation breathing, and vitamin supplements help with panic. Caffeine, alcohol, and sweets do not. I knew that already, but this suggests <em>which treatments or dietary factors have greatest impact.</em></p>
<p>Let's consider the supplements finding more closely. Of the 45 days, I took supplements on all but two. It's <em>dangerous</em> to draw any conclusions from a feature for which there are very few negative samples. This points out some important points about data analysis:</p>
<ul>
<li><strong>Know your data</strong>: otherwise, you may <em>ascribe undue meaning to outliers or noise.</em></li>
<li><strong>Know your features:</strong> supplements are probably not a good feature here. A <em>feature inclusion threshold</em> on number of positive and negative samples might be helpful here.</li>
<li><strong>Beware magic:</strong> even when you understand their inner workings, <em>machine learning algorithms can produce results that are difficult to interpret.</em></li>
</ul>
<h2 id="up-next">Up Next <a class="direct-link" href="#up-next">#</a></h2>
<p>In my next post, I look at a panic recovery dataset gathered using <a href="https://github.com/candu/qs-counters">qs-counters</a>, a simple utility I built to reduce friction in self-tracking. I perform these same three analyses on the <a href="https://github.com/candu/quantified-savagery-files/tree/master/Panic/qs-counters">qs-counters dataset</a>, then compare it to the <a href="https://github.com/candu/quantified-savagery-files/tree/master/Panic/recovery-journal">recovery-journal dataset</a>.</p>


<hr>
<ul><li>Next: <a href="/posts/qs/2012-10-14-self-tracking-for-panic-another-dataset/">Self-Tracking for Panic: Another Dataset</a></li><li>Previous: <a href="/posts/qs/2012-10-08-self-tracking-for-panic-a-deeper-look/">Self-Tracking For Panic: A bash-ful Look At Some Data</a></li>
</ul>

    </main>

    <footer></footer>

    <!-- Current page: /posts/qs/2012-10-09-self-tracking-for-panic-an-even-deeper-look/ -->
  </body>
</html>
